---
contributors: pitt500
---

Object Capture employs cutting-edge computer vision technologies to create a lifelike 3D model from a series of images taken at various angles. 
This technology was first introduced for macOS in WWDC21 but now is available for iOS 17 and above. If you want a recap of Object Capture for macOS, check out this [link](https://www.wwdcnotes.com/notes/wwdc21/10076/).

To demonstrate this new capability for iOS, Apple is providing a sample app that you can use to learn and adapt to your own apps. Follow this [link](https://developer.apple.com/documentation/realitykit/guided-capture-sample) to download the project and read the explanation below to understand how to capture objects and how this sample app works internally.

# System Requirements
The sample cannot run on a simulator, if you want to try it out, you will meet the following requirements:
- iOS or iPadOS 17.
- iPhone 12 Pro, iPad Pro 2021, or any later device with LiDAR Sensor.
> You can even compile the sample app on a device that doesn't meet those requirements, but it will crash when attempting to start capturing

# Steps to scan objects using the demo app
- Open the app and point it at the object.
- You will see an automatic bounding box generated before starting capturing. You can adjust the edges to specify the scanning area.
- The app will guide you through the regions that need to scan more images.
- You can also see feedback from the app to help you capture the best quality shots.
- After finishing the first scanning, you can flip the object to capture the bottom (if needed).
- The app will require two more scan rounds, after finishing all, the app will start generating the 3D object model. You will have to wait a few minutes.
- Once the app completed the processing, it will provide a visualization of your new 3D model.

# Support more objects with LiDAR
- LiDAR Scanner has received improvements to scan and reconstruct low-texture objects, like this chair image:
  (Show Chair)
- Although low-texture objects are supported, some objects are still challenging to scan. Avoid objects that are:
  - Reflective
  - Transparent
  - Too-thin structures

# Flippable Objects
- Textured and rigid objects are great to be scanned from the bottom, but some others are not recommended, for example, objects with repetitive texture, deformable or textureless.
(Image to flippable objects)
- It's recommended to diffuse lights to minimize shadow and reflections over the object's surface.

# Non-flippable Objects
- It is recommended to scan them from 3 heights and use a textured background where they can stand out.
(Image to mug)

# Object Capture API
Object Capture API has two steps: 
1. Image Capture
2. Model Reconstruction

# Image Capture API
- It has two parts:
  - A session from `ObjectCaptureSession` that allows you to object and control the flow of a state machine during the image capture.
  - A SwiftUI View `ObjectCaptureView` that displays the camera feed and automatically adapts the UI elements based on the session's state. 
- A session has the following transition states:
  - Initializing
  - Ready
  - Detecting
  - Capturing
  - Finishing
  - Completed
  - Failed (For errors)
 
## Adding Capture Object API to your app
- To get started, you need to first import RealityKit and SwiftUI, then create a session object:
  ```swift
  import RealityKit
  import SwiftUI 
  
  var session = ObjectCaptureSession()
  ```
  Since that `ObjectCaptureSession` is a reference type, we need to keep it alive through `@StateObject` until completing the capturing process.
- We continue calling `start` method with a directory telling the session where to store the captured images:
  ```swift
  var configuration = ObjectCaptureSession.Configuration()
  configuration.checkpointDirectory = getDocumentsDir().appendingPathComponent("Snapshots/")
  
  session.start(imagesDirectory: getDocumentsDir().appendingPathComponent("Images/"),
                configuration: configuration)
  ```
  You can add a checkpoint directory that can be used later to speed up the reconstruction process.
  Once this call is done, it will move to `ready` state.
- Next, we create an ObjectCaptureView with a session. This view must be contained inside another SwiftUI view.
  ```swift
  import RealityKit
  import SwiftUI
  
  struct CapturePrimaryView: View {
      var body: some View {
          ZStack {
              ObjectCaptureView(session: session)
          }
      }
  }
  ```
  `ObjectCaptureView` always display a UI corresponding to the current state in the session.
- To start detecting the object, you need to call in the UI `session.startDetecting()`:
  ```swift
  var body: some View {
      ZStack {
          ObjectCaptureView(session: session)
          if case .ready = session.state {
              CreateButton(label: "Continue") { 
                  session.startDetecting() 
              }
          }
      }
  }
  ```
  This state will show the bounding box to adjust the capture edges.
    - If you want to capture a different object instead, you can call `session.resetDetection()` to go back to `ready` state:
    ```swift
    Button {
      session.resetDetection()
    } label: {
      Text("Reset")
    }
    ```
- Once you are done adjusting bounding box, call `session.startCapturing` from the UI to move to next state:
  ```swift
    var body: some View {
      ZStack {
          ObjectCaptureView(session: session)
          if case .ready = session.state {
              CreateButton(label: "Continue") { 
                  session.startDetecting()
              }
          } else if case .detecting = session.state {
              CreateButton(label: "Start Capture") { 
                  session.startCapturing()
              }
          }
      }
  }
  ```
  The session will automatically takes images while you slowly move around the object.`ObjectCaptureView`will provide a dial indicating the areas that need scanning.
- Once scanning is complete, `session.userCompletedScanPass` is set to `true`:
  ```swift
  var body: some View {
      if session.userCompletedScanPass {
          VStack {
          }
      } else {
          ZStack {
              ObjectCaptureView(session: session)
          }
      }
  }
  ```
- session is capable to detect if the scanned object needs to flip to scan parts not visible by the camera yet, for example, the bottom. You can figure that out through `ObjectCaptureSession.Feedback` and calling `session.feedback.contains(.objectNotFlippable)`:
  ```swift
  if !session.feedback.contains(.objectNotFlippable) && !hasIndicatedFlipObjectAnyway {
      session.beginNewScanPass()
  } else {
      session.beginNewScanPassAfterFlip()
  }
  ```
  If the user decides to flip the object, then `beginNewScanPassAfterFlip()` will set the session to `ready` state to adjust the bounding box to the new orientation. Otherwise, we must use `beginNewScanPass()` to keep in `capturing` state and start capturing from different heights keeping the same bounding box.
  > Note: The sample app is recommending to complete the three scan passes before finishing.

- Once the three scans are done, the sample app will provide a button to end the session calling `session.finish()`. This will mutate the state to `finishing`:
  ```swift
  var body: some View {
      if session.userCompletedScanPass {
          VStack {
              CreateButton(label: "Finish") {
                  session.finish() 
              }
          }
     } else {
          ZStack {
              ObjectCaptureView(session: session)
          }
      }
  }
  ```
  While in the `finishing` state, the session waits for all data to be saved.
- Once finished, the session will automatically move to the `completed` state. This will start the on-device reconstruction.
  > In case of an error happens during all this process, session will enter into `failed` state and a new session will need to be created.
- Optionally, you can use `ObjectCapturePointCloudView` to preview which part of the object has been scanned since it was initially placed or last flipped.
  ```swift
  var body: some View {
      if session.userCompletedScanPass {
          VStack {
              ObjectCapturePointCloudView(session: session)
              CreateButton(label: "Finish") {
                  session.finish() 
              }
          }
      } else {    
          ZStack {
              ObjectCaptureView(session: session)
          }
      }
  }
  ```

## Model Reconstruction API
Here's the code in sample app that start the reconstruction process. It will work for both iOS and macOS:
```swift
var body: some View {
  ReconstructionProgressView()
      .task { // 1
          var configuration = PhotogrammetrySession.Configuration() 
          configuration.checkpointDirectory = getDocumentsDir()
              .appendingPathComponent("Snapshots/") // 3
          let session = try PhotogrammetrySession( // 2
              input: getDocumentsDir().appendingPathComponent("Images/"),
              configuration: configuration)
          try session.process(requests: [ // 4
              .modelFile(url: getDocumentsDir().appendingPathComponent("model.usdz")) 
          ])
          for try await output in session.outputs { // 5
              switch output {
                  case .processingComplete:
                      handleComplete()
                      // Handle other Output messages here.
              }
          }
      }
}
```
1. We attach `task` modifier to `ReconstructionProgressView`.
2. We create a new `PhotogrammetrySession` and point it to the images.
3. Optionally, we can provide the same checkpoint directory that we used during the image capture to speed up the reconstruction process.
4. We then call `process` function to request a model file.
5. Finally, we await the message stream in an async loop and handle output messages as they arrive.


# Detail level on iOS
- To optimize for generating and viewing models on mobile devices, iOS only supports reduced detail level.
- The reconstruction model includes the following texture maps:
  - (Diffuse)[https://docs.cryengine.com/display/SDKDOC2/Diffuse+Maps]
  - (Ambient Occlusion)[https://en.wikipedia.org/wiki/Ambient_occlusion]
  - (Normal)[https://en.wikipedia.org/wiki/Normal_mapping]
- If you require a higher level of detail, you will have to transfer your images to macOS for reconstruction.

# Capturing for Mac
- Since 2023, Mac reconstruction also utilizes the LiDAR data we save in our images.
- By default, `ObjectCaptureSession` will stop capturing images when the reconstruction limit for the iOS device is reached. For macOS, you can allow the session to take more images than on-device reconstruction can use by setting `isOverCaptureEnabled` to `true` in session's configurations:
  ```swift
  var configuration = ObjectCaptureSession.Configuration()
  configuration.isOverCaptureEnabled = true
  
  session.start(imagesDirectory: getDocumentsDir().appendingPathComponent("Images/"),
                configuration: configuration)
  ```
- These additional shots will not be used for on-device reconstruction, but they are stored in the Images folder.

# Model Reconstruction in Mac
- You don't need any single line of code, just import the images to [Reality Composer Pro](https://developer.apple.com/videos/play/wwdc2023/10083/), choose a detail level and get your model!
(Image of Reality Composer Pro)

# Reconstruction Enhancements
- Increased performance and image quality on Mac.
- In addition to progress percent, it now provides estimated processing time.
- You can now request a high-quality pose for each image. Each pose includes the estimated position and orientation of the camera for that image.
  (Image to pose output)
- To get the poses, add `.poses` to the `process()` function call and handle them then they arrive in the messages stream:
  ```swift
  try session.process(requests: [ 
      .poses 
      .modelFile(url: modelURL),
  ])
  for try await output in session.outputs {
      switch output {
      case .poses(let poses):
          handlePoses(poses)
      case .processingComplete:
          handleComplete()
      }
  }
  ```
- The API now provides a new custom detail level to control the amount of mess decimation, the texture map resolution, format and which texture maps should be included.
(Image of level of detail)










If this is your first note, please make sure to check out the contributing guide https://github.com/WWDCNotes/Content/blob/main/CONTRIBUTING.md
