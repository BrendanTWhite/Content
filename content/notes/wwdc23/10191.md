---
contributors: pitt500
---

Object Capture employs cutting-edge computer vision technologies to create a lifelike 3D model from a series of images taken at various angles. 
This technology was first introduced for macOS (more details in this [link](https://www.wwdcnotes.com/notes/wwdc21/10076/) from WWDC21) but now will be available in iOS 17 and above.

Apple is providing a sample app to demonstrate this capability. Follow this [link](https://developer.apple.com/documentation/realitykit/guided-capture-sample) to download the project.

# System Requirements
- iOS or iPadOS 17.
- A device with LiDAR scan.
- iPhone 12 Pro (or iPad Pro 2021) and later with LiDAR Sensor.

# Steps to scan objects using the demo app
- Open the app and point it at the object.
- You will see an automatic bounding box generated before starting capturing. You can adjust the edges to specify the scanning area.
- The app will guide you through the regions that need to scan more images.
- You can also see feedback from the app to help you capture the best quality shots.
- After finishing the first scanning, you can flip the object to capture the bottom (if needed).
- The app will require two more scan rounds, after finishing all, the app will start generating the 3D object model. You will have to wait a few minutes.
- Once the app completed the processing, it will provide a visualization of your new 3D model.

# Support more objects with LiDAR
- LiDAR Scanner has received improvements to scan and reconstruct low-texture objects, like this chair image:
  (Show Chair)
- Although low-texture objects are supported, some objects are still challenging to scan. Avoid objects that are:
  - Reflective
  - Transparent
  - Too-thin structures

# Flippable Objects
- Textured and rigid objects are great to be scanned from the bottom, but some others are not recommended, for example, objects with repetitive texture, deformable or textureless.
(Image to flippable objects)
- It's recommended to diffuse lights to minimize shadow and reflections over the object's surface.

# Non-flippable Objects
- It is recommended to scan them from 3 heights and use a textured background where they can stand out.
(Image to mug)

# Object Capture API
Object Capture API has two steps: 
1. Image Capture
2. Model reconstruction

## Image Capture API
- It has two parts:
  - A session from ObjectCaptureSession that allows you to object and control the flow of a state machine during the image capture.
  - A SwiftUI View ObjectCaptureView that displays the camera feed and automatically adapts the UI elements based on the session's state. 
- A session has the following transition states:
  - Initializing
  - Ready
  - Detecting
  - Capturing
  - Finishing
 
## Adding Capture Object API to your app
- To get started, you need to first import RealityKit and SwiftUI, then create a session object:
  ```swift
  import RealityKit
  import SwiftUI 
  
  var session = ObjectCaptureSession()
  ```
  Since that `ObjectCaptureSession` is a reference type, we need to keep it alive through `@StateObject` until completing the capturing process.
- We continue calling `start` method with a directory telling the session where to store the captured images:
  ```swift
  var configuration = ObjectCaptureSession.Configuration()
  configuration.checkpointDirectory = getDocumentsDir().appendingPathComponent("Snapshots/")
  
  session.start(imagesDirectory: getDocumentsDir().appendingPathComponent("Images/"),
                configuration: configuration)
  ```
  You can add a checkpoint directory that can be used later to speed up the reconstruction process.
  Once this call is done, it will move to `ready` state.
- Next, we create an ObjectCaptureView with a session. This view must be contained inside another SwiftUI view.
  ```swift
  import RealityKit
  import SwiftUI
  
  struct CapturePrimaryView: View {
      var body: some View {
          ZStack {
              ObjectCaptureView(session: session)
          }
      }
  }
  ```
  `ObjectCaptureView` always display a UI corresponding to the current state in the session.
- To start detecting the object, you need to call in the UI `session.startDetecting()`:
  ```swift
  var body: some View {
      ZStack {
          ObjectCaptureView(session: session)
          if case .ready = session.state {
              CreateButton(label: "Continue") { 
                  session.startDetecting() 
              }
          }
      }
  }
  ```
  This state will show the bounding box to adjust the capture edges.
    - If you want to capture a different object instead, you can call `session.resetDetection()` to go back to `ready` state:
    ```swift
    Button {
      session.resetDetection()
    } label: {
      Text("Reset")
    }
    ```
- Once you are done adjusting bounding box, call `session.startCapturing` from the UI to move to next state:
  ```swift
    var body: some View {
      ZStack {
          ObjectCaptureView(session: session)
          if case .ready = session.state {
              CreateButton(label: "Continue") { 
                  session.startDetecting()
              }
          } else if case .detecting = session.state {
              CreateButton(label: "Start Capture") { 
                  session.startCapturing()
              }
          }
      }
  }
  ```
  The session will automatically takes images while you slowly move around the object.`ObjectCaptureView`will provide a dial indicating the areas that need scanning.
- Once scanning is complete, `session.userCompletedScanPass` is set to `true`:
  ```swift
  var body: some View {
      if session.userCompletedScanPass {
          VStack {
          }
      } else {
          ZStack {
              ObjectCaptureView(session: session)
          }
      }
  }
  ```
- session is capable to detect if the scanned object needs to flip to scan parts not visible by the camera yet, for example the bottom. You can figure that out through `ObjectCaptureSession.Feedback` and calling `session.feedback.contains(.objectNotFlippable)`:
  ```swift
  if session.feedback.contains(.objectNotFlippable) {
      session.beginNewScanPass()
  } else {
      session.beginNewScanPassAfterFlip()
  }
  ```
  Depending if the object is flippable or not, you can start a new scanning to improve the reconstruction by calling either `beginNewScanPassAfterFlip()` or `beginNewScanPass()` respectively. The sample app is recommending to complete the three scan passes before finishing.

   
  

## Model Reconstruction API










If this is your first note, please make sure to check out the contributing guide https://github.com/WWDCNotes/Content/blob/main/CONTRIBUTING.md
